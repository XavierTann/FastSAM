{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5001\n",
      " * Running on http://192.168.1.10:5001\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug: * Restarting with stat\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n",
      "    app.initialize(argv)\n",
      "  File \"/opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages/traitlets/config/application.py\", line 118, in inner\n",
      "    return method(app, *args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 692, in initialize\n",
      "    self.init_sockets()\n",
      "  File \"/opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 331, in init_sockets\n",
      "    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n",
      "  File \"/opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 253, in _bind_socket\n",
      "    return self._try_bind_socket(s, port)\n",
      "  File \"/opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 229, in _try_bind_socket\n",
      "    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n",
      "  File \"/opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages/zmq/sugar/socket.py\", line 302, in bind\n",
      "    super().bind(addr)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 564, in zmq.backend.cython.socket.Socket.bind\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 28, in zmq.backend.cython.checkrc._check_rc\n",
      "zmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9023')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import logging\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from samgeo.fast_sam import SamGeo  # Import the SamGeo class for raster-to-vector conversion\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Configure CORS more specifically\n",
    "CORS(app, resources={\n",
    "    r\"/test\": {\n",
    "        \"origins\": [\"http://localhost:3000\"],  # Adjust this to match your frontend URL\n",
    "        \"methods\": [\"POST\"],\n",
    "        \"allow_headers\": [\"Content-Type\"]\n",
    "    }\n",
    "})\n",
    "\n",
    "@app.route('/test', methods=['POST'])\n",
    "def test():\n",
    "    try:\n",
    "        \n",
    "        data = request.get_json()\n",
    "        if data is None:\n",
    "            return jsonify({\"status\": \"error\", \"message\": \"No JSON data received\"}), 400\n",
    "        \n",
    "        signed_url = data.get('signedUrl')\n",
    "            \n",
    "        fileName = data.get('fileName')\n",
    "\n",
    "        textPrompt = data.get('textPrompt')\n",
    "\n",
    "        listOfPoints= data.get('listOfPoints')\n",
    "\n",
    "        # Download the file from the signed URL\n",
    "        response = requests.get(signed_url, stream=True)\n",
    "        \n",
    "    \n",
    "        output_folder = \"./output\"\n",
    "\n",
    "        original_file_path = os.path.join(output_folder, fileName)\n",
    "\n",
    "        # Download the file from the public URL\n",
    "        response = requests.get(signed_url)\n",
    "\n",
    "        # Save the file to the output folder\n",
    "        with open(original_file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        output_raster = './output/output_raster.tif'  # Output path for the raster mask (GeoTIFF)\n",
    "        output_vector = './output/output_vector.geojson'  # Output path for the vector file (GeoJSON)\n",
    "\n",
    "\n",
    "        sam = SamGeo(model=\"FastSAM-x.pt\")  # LangSAM for text-based segmentation\n",
    "\n",
    "\n",
    "        \n",
    "        # point prompt\n",
    "        # image_path = './images/cat.jpg'\n",
    "        sam.set_image(original_file_path)\n",
    "        positive_points = [[450, 300], [400, 200], [500, 200], [450, 450]] \n",
    "        positive_points = listOfPoints\n",
    "        negative_points = [[100, 100], [550, 600], [300, 350]]  # Areas to exclude (background and red object)\n",
    "        # Define the labels for the points: 1 for positive points, 0 for negative points\n",
    "        point_labels = [1, 1, 1, 1, 0, 0, 0]  # 1: Positive, 0: Negative\n",
    "        sam.point_prompt(points=positive_points + negative_points, pointlabel=point_labels)\n",
    "        # Save the result as an image (e.g., PNG)\n",
    "        output_image = \"./output/output_segmentation_with_points.png\"\n",
    "        sam.show_anns(output_image)\n",
    "\n",
    "\n",
    "        # # Perform segmentation based on the text prompt using langsam\n",
    "        # sam.predict(output_file_path, textPrompt, box_threshold=0.24, text_threshold=0.24)\n",
    "\n",
    "        # # Save the result to a raster file (GeoTIFF)\n",
    "        # sam.show_anns(\n",
    "        #     cmap=\"Greys_r\", \n",
    "        #     add_boxes=False, \n",
    "        #     alpha=1, \n",
    "        #     title=\"Segmentation Result\", \n",
    "        #     blend=False, \n",
    "        #     output=output_raster\n",
    "        # )\n",
    "\n",
    "        # Convert the raster result to vector format\n",
    "        sam.raster_to_vector(output_raster, output_vector)\n",
    "\n",
    "\n",
    "        # sam = SamGeo(model=\"FastSAM-x.pt\")  \n",
    "\n",
    "        # sam.set_image(output_file_path)\n",
    "\n",
    "\n",
    "        # sam.everything_prompt(output=output_raster) \n",
    "\n",
    "\n",
    "        # sam.raster_to_vector(output_raster, output_vector) \n",
    "        \n",
    "        return jsonify({\n",
    "            \"status\": \"success\",\n",
    "            \"received_signed_url\": signed_url\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error processing request\")\n",
    "        return jsonify({\n",
    "            \"status\": \"error\", \n",
    "            \"message\": str(e),\n",
    "            \"type\": str(type(e).__name__)\n",
    "        }), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5001, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/xc/f09q_z6943z0mvqw0lrn19dw0000gp/T/pip-req-build-e4kxf9tr\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/xc/f09q_z6943z0mvqw0lrn19dw0000gp/T/pip-req-build-e4kxf9tr\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from clip==1.0) (6.3.0)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from clip==1.0) (24.1)\n",
      "Requirement already satisfied: regex in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from clip==1.0) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from clip==1.0) (4.66.5)\n",
      "Requirement already satisfied: torch in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from clip==1.0) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from clip==1.0) (0.19.1)\n",
      "Requirement already satisfied: wcwidth in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torch->clip==1.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torch->clip==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torch->clip==1.0) (2024.9.0)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torchvision->clip==1.0) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torchvision->clip==1.0) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from jinja2->torch->clip==1.0) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "import clip\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/xc/f09q_z6943z0mvqw0lrn19dw0000gp/T/pip-req-build-3r6wrj0t\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/xc/f09q_z6943z0mvqw0lrn19dw0000gp/T/pip-req-build-3r6wrj0t\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from clip==1.0) (6.3.0)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from clip==1.0) (24.1)\n",
      "Requirement already satisfied: regex in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from clip==1.0) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from clip==1.0) (4.66.5)\n",
      "Requirement already satisfied: torch in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from clip==1.0) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from clip==1.0) (0.19.1)\n",
      "Requirement already satisfied: wcwidth in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torch->clip==1.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torch->clip==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torch->clip==1.0) (2024.9.0)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torchvision->clip==1.0) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from torchvision->clip==1.0) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from jinja2->torch->clip==1.0) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/xaviertan/Documents/Nika/Code/FastSAM/images/cat.jpg: 1024x576 38 objects, 1284.8ms\n",
      "Speed: 3.3ms preprocess, 1284.8ms inference, 17.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IHDR' 16 13\n",
      "DEBUG:PIL.PngImagePlugin:STREAM b'IDAT' 41 32768\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m\n\u001b[1;32m     34\u001b[0m sam\u001b[38;5;241m.\u001b[39mset_image(image_path)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# #\"everything\" prompt\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# sam.everything_prompt(output=output_raster)  # Output the mask as a GeoTIFF\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# text prompt\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43msam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjust the eyes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_raster\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m sam\u001b[38;5;241m.\u001b[39mshow_anns(output_raster\u001b[38;5;241m.\u001b[39mpng)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# # point prompt\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# positive_points = [[450, 300], [400, 200], [500, 200], [450, 450]]  # Areas to include (head, ears, body)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# negative_points = [[100, 100], [550, 600], [300, 350]]  # Areas to exclude (background and red object)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# plt.axis('off')\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/FastSAM/lib/python3.9/site-packages/samgeo/fast_sam.py:145\u001b[0m, in \u001b[0;36mSamGeo.text_prompt\u001b[0;34m(self, text, output, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Segment the image with the text prompt. Adapted from\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03mhttps://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L439\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    output (str, optional): The path to save the output image. Defaults to None.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m prompt_process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_process\n\u001b[0;32m--> 145\u001b[0m ann \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations \u001b[38;5;241m=\u001b[39m ann\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Nika/Code/FastSAM/fastsam/prompt.py:447\u001b[0m, in \u001b[0;36mFastSAMPrompt.text_prompt\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    445\u001b[0m format_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    446\u001b[0m cropped_boxes, cropped_images, not_crop, filter_id, annotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crop_image(format_results)\n\u001b[0;32m--> 447\u001b[0m clip_model, preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mclip\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViT-B/32\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    448\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(clip_model, preprocess, cropped_boxes, text, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    449\u001b[0m max_idx \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39margsort()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clip' is not defined"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "clip.available_models()\n",
    "\n",
    "from samgeo.fast_sam import SamGeo  # Import the SamGeo class for raster-to-vector conversion\n",
    "from samgeo import tms_to_geotiff\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Specify the model path and image path\n",
    "model_path = './weights/FastSAM-x.pt'  # Path to the FastSAM model\n",
    "output_raster = './output/output_raster.jpg'  # Output path for the raster mask (GeoTIFF)\n",
    "output_vector = './output/output_vector.geojson'  # Output path for the vector file (GeoJSON)\n",
    "\n",
    "\n",
    "sam = SamGeo(model=\"FastSAM-x.pt\")  # Initialize SamGeo with the FastSAM model\n",
    "\n",
    "\n",
    "# Define the local output path\n",
    "output_folder = \"./output\"\n",
    "output_file_path = os.path.join(output_folder, \"initial_file.tif\")\n",
    "\n",
    "\n",
    "image_path = './images/cat.jpg'\n",
    "sam.set_image(image_path)\n",
    "\n",
    "\n",
    "\n",
    "# #\"everything\" prompt\n",
    "# sam.everything_prompt(output=output_raster)  # Output the mask as a GeoTIFF\n",
    "\n",
    "# text prompt\n",
    "sam.text_prompt(text = \"just the eyes\", output = output_raster)\n",
    "sam.show_anns(output_raster.png)\n",
    "\n",
    "# # point prompt\n",
    "# positive_points = [[450, 300], [400, 200], [500, 200], [450, 450]]  # Areas to include (head, ears, body)\n",
    "# negative_points = [[100, 100], [550, 600], [300, 350]]  # Areas to exclude (background and red object)\n",
    "\n",
    "# # Define the labels for the points: 1 for positive points, 0 for negative points\n",
    "# point_labels = [1, 1, 1, 1, 0, 0, 0]  # 1: Positive, 0: Negative\n",
    "# sam.point_prompt(points=positive_points + negative_points, pointlabel=point_labels)\n",
    "# # Save the result as an image (e.g., PNG)\n",
    "# output_image = \"cat_segmentation_with_points.png\"\n",
    "# sam.show_anns(output_image)\n",
    "\n",
    "# Step 3: Convert the raster segmentation result to a vector format\n",
    "# sam.raster_to_vector(output_raster, output_vector)  # Convert GeoTIFF to GeoJSON\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastSAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
